DAG:
    dag_id: dag_transfer_files
    args:
        description: this is a pipeline for transfer files
        max_active_runs: 1
        schedule_interval: 5 0 * * *
        catchup: !!bool True
        default_args:
            depends_on_past: !!bool True
            start_date: !!python/object/apply:datetime.datetime [2025, 12, 10]
            email_on_failure: !!bool False
            email_on_retry: !!bool False
            retries: 1
            retry_delay: !!python/object/new:datetime.timedelta
              kwds: {minutes: 1}
            sla: !!python/object/new:datetime.timedelta
              kwds: {hours: 1}
            execution_timeout: !!python/object/new:datetime.timedelta
              kwds: {hours: 2}
    tasks:
        start_sync:
            operator: !!python/name:airflow.operators.dummy.DummyOperator
            upstream: []

        {% for batch_id in range(config.num_batches) %}
        sync_batch_{{ batch_id }}:
            operator: !!python/name:operators.file_sync_operator.FileSyncOperator
            args:
                source_type: "{{ config.source_type }}"
                target_type: "{{ config.target_type }}"
                source_conn_id: "{{ config.source_conn_id }}"
                target_conn_id: "{{ config.target_conn_id }}"
                source_path: "{{ config.source_path }}"
                modulo_id: {{ batch_id }}
                num_batches: {{ config.num_batches }}
                chunk_size: {{ config.chunk_size }}
                {% if config.get('transformation_func') %}
                transformation_func: !!python/name:{{ config.transformation_func }}
                {% endif %}
            upstream:
                - start_sync
        {% endfor %}

        end_sync:
            operator: !!python/name:airflow.operators.dummy.DummyOperator
            upstream: [
                {%- for batch_id in range(config.num_batches) -%}
                sync_batch_{{ batch_id }}{{ "," if not loop.last else "" }}
                {%- endfor -%}
            ]