DAG:
    dag_id: dag_transfer_files
    args:
        description: this is a pipeline for transfer files
        max_active_runs: 1
        schedule_interval: !!python/none
        catchup: !!bool True
        default_args:
            depends_on_past: !!bool True
            start_date: !!python/object/apply:datetime.datetime [2025, 12, 10]
            email_on_failure: !!bool False
            email_on_retry: !!bool False
            retries: 1
            retry_delay: !!python/object/new:datetime.timedelta
              kwds: {minutes: 1}
            sla: !!python/object/new:datetime.timedelta
              kwds: {hours: 1}
            execution_timeout: !!python/object/new:datetime.timedelta
              kwds: {hours: 2}
    tasks:
        start_sync:
            operator: !!python/name:airflow.operators.dummy.DummyOperator
            upstream: []

        {% for batch in config.batches %}
        sync_batch_{{ batch.batch_id }}:
            operator: !!python/name:operators.file_sync_operator.FileSyncOperator
            args:
                source_type: "{{ config.source_type }}"
                target_type: "{{ config.target_type }}"
                source_conn_id: "{{ config.source_conn_id }}"
                target_conn_id: "target_sftp_conn"
                source_files: {{ batch.files }}
                source_path: "{{ config.path }}"
                target_path: "{{ config.path }}"
                chunk_size: {{ config.chunk_size }}
            upstream:
                - start_sync
        {% endfor %}

        end_sync:
            operator: !!python/name:airflow.operators.dummy.DummyOperator
            upstream: [
                {%- for batch in config.batches -%}
                sync_batch_{{ batch.batch_id }}{{ "," if not loop.last else "" }}
                {%- endfor -%}
            ]